\documentclass[uplatex,a4paper,11pt,dvipdfmx]{jreport}

\usepackage{is-thesis}
\usepackage{graphicx}
\usepackage{bm}
% --------------------------------------------------
% 論文情報（伏せ字）
% --------------------------------------------------
\titleJP{機械学習を用いた転倒検知システムの開発}
\subtitleJP{SDNを用いた動的ルーティング制御の一検討}
\authorJP{堀川 風花}             % ← 氏名
\thesisDate{2026年2月}

\begin{document}

% --------------------------------------------------
% 表紙
% --------------------------------------------------
\makethesistitlepage

% --------------------------------------------------
% 目次（ローマ数字）
% --------------------------------------------------
\beginfrontmatter
\makethesistoc

% --------------------------------------------------
% 本文（アラビア数字）
% --------------------------------------------------
\beginmainmatter

% --------------------------------------------------
% 第1章 序論
% --------------------------------------------------
\chapter{序論}

大学キャンパスや企業内LANではネットワーク利用者の増加に伴い，
ネットワークトラフィックが急増している．
その結果，混雑による遅延やパケットロスが発生し，
通信品質の低下が課題となっている．

Software Defined Networking（SDN）は，
ネットワーク機器の制御プレーンとデータプレーンを分離することで，
集中管理と柔軟な制御を可能とする技術である．
SDN を活用すれば動的ルーティングによりトラフィック最適化が期待できる．

\section{背景}

従来のネットワークでは静的ルーティングに依存しており，
トラフィック集中時の動的な回避が困難である．

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

\section{研究目的}

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

本研究の目的は、
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

% --------------------------------------------------
% 第2章 関連研究
% --------------------------------------------------
\chapter{関連研究・技術}
\section{姿勢推定を用いた転倒検知}
AlphaPose、OpenPose、BlazePose、OpenPifPaf、HRNetを用いてフレームデータから人間の骨格を抽出し、
機械学習モデルを構築した.具体的にはRNN、LSTM、ST-GCN、Vision Transformersなどの
機械学習モデルを作成し,転倒イベントを識別した．さらに、高度な姿勢推定フレームワークであるViTPoseとTrans-Poseを導入することで、
調査範囲を拡大した．実験は3つの異なるデータセットを用いて、転倒検知技術の全体的な評価をしていた.
様々なポーズ抽出器について詳細な分析を行い,各手法で生成された明確なスケルトンポイント（骨格位置）を機械学習モデルを学習させた.

ここではフレームに対する人の各関節の絶対位置であるスケルトン情報の前処理やスライディングウィンドウに触れたい.
この論文では前処理として正規化手法や絶対位置からの相対位置変換を紹介している.

姿勢検出手法は、環境ノイズ,カメラの位置、屋内物体からの干渉の影響を受けやすい.そのため、これらの前処理により、
特に複雑な屋内環境において,姿勢検出の精度と信頼性を大幅に向上させる.

最小-最大正規化を適用してデータを区間[0, 1]にスケーリングし、モデルの収束を早めてモデルの精度を向上させます．
新しいスケールデータは式 に従って計算される．

\begin{equation}
X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}} \in  \lbrack 0,1 \rbrack
\label{eq:scale_norm}
\end{equation}

元のデータセットの値はXで表され、最小値は$X_{min}$で表され、最大値は$X_{max}$で表される．

ただし、データセットには欠損値を含むインスタンスが含まれている．
全てのグループに欠損データがあるわけではないが、値が0である場合は$X_{min}$も0であることを示す．
この元のデータ分布の変化により、不確実な属性が生じる．したがって、式 
および式(7)で概説した正規化手法の使用を提案する．

\begin{equation}
x_c^f=\frac{W}{2}\\
y_c^f=\frac{H}{2}
\label{eq:body_center}
\end{equation}

元の座標位置を, $x_n^f$と$y_n^f$座標系における同等の位置に変換し、比較性を高める．
値が0の点は欠損点と指定され、計算から除外される．
欠損点が存在する場合、その位置には計算された中心点が代入される．
変位が必要な距離（$x_{dis}$, $y_{dis}$）は式(8)および式(9)によって定義される．

\begin{equation}
x_{dis}^f=x_8^f-x_c^f\\
y_{dis}^f=y_8^f-y_c^f
\label{eq:x_{dis}^f_y_{dis}^f}
\end{equation}

変位は第8関節点($x_8^f$, $y_8^f$)の中心点へ、基準点は股関節中心円周($x_c^f$,$y_c^f$)となる．
変位値が負の場合、物体は右方向へ移動する．変位値がゼロより大きい場合、オブジェクトは左に移動する．
相対位置への変位を計算するための接合点の更新座標（$rx_{n}^f$, $ry_{n}^f$）は以下のように与えられる：
\begin{equation}
rx_{n}^f=(x_n^f-x_{dis}^f)\\
ry_{n}^f=(y_n^f-y_{dis}^f)
\label{eq:rx_n^f_ry_n^f}
\end{equation}
相対位置正規化手法には二つの明確な利点がある．
第一に、元のデータ分布を保持すること．
第二に、欠損データの同時計算を不要とすること．
さらにこの処理は冗長な特徴量を排除する．
特筆すべきは、横方向移動や近接性といった人間の移動特性が骨格位置を安定的に維持する点である．
この特性によりモデルに連続的な動きが与えられ、学習プロセスに有益であることが実証されている．

転倒と非転倒事象の判別において、特定の骨格関節は識別特性を持たないとみなされる．
処理の効率化と明瞭化のため、これらの冗長な関節は除去される．
最終的に保持される訓練用関節セットには、鼻、肩、肘、手首、首、股関節、膝、足首が含まれる．

BlazePoseは主に2D画像特徴量に依存して骨格を識別するため、これらの特徴量が欠落すると
不安定性や予測誤差が生じます．人体が隠蔽されたり形状が不明瞭だったり、上下肢が覆われている場合、
骨格構築時にデータ損失や不正確さが発生する可能性があります．
提案手法では、線形補間を用いて欠損データを補正することでこの問題に対処する．
十分なデータセットを確保するため、100枚の画像を集約し、欠損値のない初期データを用いて
後続画像の欠損部分を補完する．この補間プロセスにより、元のデータセットから
予測データ値を導出する．
ただし、初期関節点の数が不十分な場合、予測値と実際の欠損値に大きな乖離が生じる可能性がある．
図10は検出されなかった骨格関節の事例を示す．線形補間では、
2つの既知データ点を用いてそれらの間の傾きを計算し、Y（式(12)）のおおよその値を推定する．

\begin{equation}
Y=Y_0+\frac{Y_1-Y_0}{X_{1}-Y_{0}} (X-X_0)
\label{eq:predicted_y}
\end{equation}

推定値と実測値の差異は、遷移点が完全な状態では統計的に有意な差を示さない．
RP正規化前、データセットには22.38\%の重要点の欠落が認められた．
RP正規化後、この欠落率は13.4\%に減少した．
さらに、補間処理とRP正規化を経て、欠落する重要点の発生率は全データセットのわずか1.8\%にまで低下した．

動画から機械学習モデルに入力するための時系列データを作成するため、
まずUR-Fall、UPFall、またはLe2iデータセットから個々の人体骨格を取得する．
次のステップでは、AlphaPose、OpenPose、BlazePose、HRnet、OpenPifPafといった
多様な姿勢抽出器を用いて人間の姿勢を推定します．
これらの姿勢抽出器は重要なキーポイントの特定に役立つ．
その後、これらのキーポイントを主要特徴量として利用するモデルを構築しする．
トレーニング過程では、転倒または非転倒活動を示すシーケンスの予測を可能にするため、
様々な機械学習モデルを活用する．この手法により、多様な姿勢推定技術と機械学習モデルを活用し、
様々なデータセットにおける転倒検出の探索・分析が可能となる．
フレームシーケンスを決定するため、スライディングウィンドウが連続する骨格特徴を結合する．
この手法では、ウィンドウサイズは動画レート（fps）にウィンドウサイズ（$swl = fps \times $秒）
を乗じた画像シーケンスをカバーするように設定される．例として、1秒間隔で25フレーム/秒の動画の場合、
各ウィンドウは25フレーム（swl = 25）をカバーする．
各ウィンドウは前のウィンドウの右隣のフレームに位置し、動画の最終フレームがウィンドウでカバーされるまで
この配置が続く．スライディングウィンドウの総数は、以下の式を用いて決定できる：

\begin{equation}
sw_m=FrameVideo-swl+1
\label{eq:number_of_sw}
\end{equation}

本手法における総分岐数は、動画フレーム処理に「スライディングウィンドウ」と呼ばれる概念を用い、swで表される．
動画の総フレーム数はFrameVideoで表される．ここでFrameVideoは動画内の総フレーム数を表す．
各スライディングウィンドウは、個人の動作を捕捉する連続したフレーム群を包含する．この構成により、
各スライディングウィンドウには様々な活動に従事する個人の骨格データが含まれることが保証される．
(1,swl, n) 形式の各フレームは、n個の識別された骨格で構成され、ここで n は検出された骨格の数を示す．
スライディングウィンドウ手法に関して、我々のアプローチでは、スライディングウィンドウ内の各フレームセットが
分類推論を受ける．通常、各スライディングウィンドウから単一の分類結果が生成される．
1つの分類結果に寄与するフレーム数（またはHPEモデルが返す関節キーポイントのセット数）は、
ウィンドウサイズ（swl）と一致します．swlは、時間（秒）に動画フレームレート（fps）を乗じた値です．
したがって、推論レートが25fpsの場合、各スライディングウィンドウのフレームセットには25フレーム、
または単一の分類出力生成に寄与する関節キーポイントのセットが含まれる．

これらのデータ前処理やスライディングウィンドウによってどの姿勢推定技術と機械学習モデルの組み合わせでもモデルの精度は90\%を超える結果となった．

この研究は本研究と似ている部分もあるが、この研究は検知できる人数は1人を想定しており、
複数人での利用は想定していなかった．

そのため、本研究ではこれらの知見を踏まえ，スケルトンデータの正規化や時系列データに適した
機械学習モデルの選定を行い、複数人でも対応できるシステムを作成する．
また、本研究では数字的なモデルの精度の値のみではなく、実際の利用を想定した際の処理遅延やモデルの転倒検知の
精度や偽陰性率を評価したい．

\section{３次元姿勢推定技術による転倒検知}
この研究では、異なるデータセット間における3D人体姿勢推定の汎化性能の低下に着目している．

RCB画像からのの3D人体姿勢推定アの研究は多く存在するが、通常、被写体、姿勢、カメラ、照明など、
多くの要因に関して多様性が制限されたデータセットで評価されているため、
実際の任意の条件（「in-the-wild」）で動作させると性能が低下すると考えられる．
カメラの視点、被写体、およびデータセット間の汎化を大幅に改善するスケール正規化手法を提案している．
追加の実験では、カメラの増減、複数のデータセットを用いた学習、提案された解剖学に基づく姿勢検証ステップの適用、
そして3D姿勢推定の基盤としてOpenPoseを使用することによる効果を調査している．
実験結果は、データの前処理（異なる姿勢推定による関節検知の結果の差を緩和するため）、
スケール正規化、そして仮想カメラの拡張が、モデルの汎化を大幅に改善する上で有用であることを示している．

スケール正規化では具体的に推定した人のスケルトンポイントの正規化や相対位置変換を行っている．
計算手順は以下の通りである．
各ポーズサンプルの絶対関節座標$p_i$は、ローカル座標系の原点にあるヒップの中心点$p_0$に対する
スケルトンの相対的な関節位置に基づいて個別に正規化された．
スケールsは、原点とN個の関節位置すべてのユークリッド距離の平均を計算することで定量化した．


\begin{equation}
s = \frac{1}{N} \sum_{i=1}^{N} \left\| \mathbf{p}_i - \mathbf{p}_0 \right\|
\label{eq:human_scale}
\end{equation}

その後、すべての関節位置座標をスケールで割ってスケルトンのサイズを変更し、
正規化されたスケールを1にした．
正規化された関節位置$p_i$は次のように計算された．

「肩〜足首の距離」など特定の部位基準ではないので、学術的に厳密な「身長正規化」とは少し異なるが、
人物の大きさに依存しない形に正規化をした．

\begin{equation}
\hat{\mathbf{p}}_i = \frac{1}{s} (\mathbf{p}_i - \mathbf{p}_0)
\label{eq:joint_norm}
\end{equation}

本研究では転倒検知を目的としており，タスク自体は異なるものの，動画から推定した関節座標を時系列データとして学習する点で共通する課題を有する．

そのため，本研究においてもRapczyńskiらの知見を参考に，人物および撮影条件に依存しない特徴抽出を実現するため，
関節座標に対してスケール正規化を適用する．
前処理として，各フレームで推定された人体のスケルトン関節位置に正規化と相対値変換が存在している．

\section{姿勢推定を用いた転倒検知}





% --------------------------------------------------
% 第3章 □□□□
% --------------------------------------------------
\chapter{原理}

\section{システム全体構成}
本研究では，姿勢推定により得られた関節座標の時系列情報を用いて，転倒動作を分類する時系列分類モデルを構築した。
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/learning_model.png}
  \caption{分類モデル作成の流れ}
  \label{fig:create_model_flow}
\end{figure}

分類モデルに学習させるデータとして以下の3つの状態に分類される1〜3秒の動画データを各クラス
700個ずつ、合計2100個の動画データを作成した。
動画は私以外の人にも協力してもらい、異なる方向や異なる角度から撮影し、なるべく多様なデータになるようにした。
姿勢推定にはyolo-poseを採用し、動画データの各フレームごとからスケルトン情報を取得する。
\begin{itemize}
 \item sit（床に座っている、しゃがみ込んでいる状態）
 \item stand（歩行している状態）
 \item fall（転倒している状態）
\end{itemize}


このスケルトン情報を入力としてあらかじめ付与したラベルを教師データとして
学習させたモデルを用いて，リアルタイム転倒検知システムを構築した。
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.25]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/system_flow.png}
  \caption{システムの流れ}
  \label{fig:system_flow}
\end{figure}
推論時には入力映像に対してYOLOv8-Poseを用いて人物検出および姿勢推定を行い，
YOLO-Trackingにより人物IDを時系列で追跡する。
各人物ごとに関節座標系列を蓄積し，30フレーム分の特徴量が揃った時点でLSTMモデルによる動作分類を行う。

% 転倒と判定された場合には，画面上に警告表示を行うとともに，外部通知（電子メール）によるアラート送信を可能とした。



% \begin{table}[tbp]
%   \centering
%   \caption{ルーティング方式の比較}
%   \label{tb:algo}
%   \begin{tabular}{lcc}
%     \hline
%     手法 & 遅延(ms) & パケットロス(\%) \\
%     \hline
%     静的ルーティング & 35 & 2.1 \\
%     提案法（SDN動的） & 18 & 0.4 \\
%     \hline
%   \end{tabular}
% \end{table}

\subsection{スライディングウィンドウ技術}
モデル学習時には長さの異なる動画を入力するためにフレームの数を30フレームに固定した。
各動画を等間隔で30フレームずつサンプリングし、一つの時系列データとして扱った。
また、姿勢推定では13個のキーポイントを取得し、各キーポイントは
x座標とy座標で表すので、各フレームは26次元のデータとなる。
つまりモデルの入力は$26（特徴次元）\times  30（時系列長）$の時系列特徴量になる。\\
推論時にはスライディング法を用いて特徴量を構成する。
ウィンドウサイズは W=30 フレームとし、各人物について直近30フレーム分の特徴量を
 FIFO（First-In First-Out）形式で保持する。
新たなフレームが入力されるたびに最新の特徴量を追加し、
ウィンドウが満たされた状態では最も古いフレームを破棄することで、連続した動作情報を維持する。
ウィンドウが30フレームに達した時点で、
その時系列特徴をLSTMモデルに入力し、stand、sit、fallといった行動クラスを推定する。
推定結果は短時間の揺らぎを抑制するため，直近5フレームの予測結果に対して多数決による平滑化処理を行った。

\subsection{LSTMを用いた転倒検知}
分類モデルには，時系列データの依存関係を学習可能な
LSTM（Long Short-Term Memory）ネットワークを用いた。
ネットワーク構成は，LSTM層（128ユニット）とLSTM層（64ユニット）を直列に配置し，
過学習抑制のためにDropout層（0.3）を各層後に挿入した。
LSTMから得られた特徴ベクトルに対し、ReLU活性化関数を用いた全結合層（32ユニット）を適用し、非線形変換を行う。
最終層にはSoftmax関数を用いた全結合層を配置し、fall,stand，sitの3クラスに
対する確率分布を出力する。
学習には，交差エントロピー損失関数とAdam最適化手法を用いた。
エポック数は30，バッチサイズは16とした。

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/lstm.png}
  \caption{LSTMモデルの構成}
  \label{fig:lstm_network}
\end{figure}


\subsection{データ前処理}
フレームからyolo-poseを使って取得したスケルトン情報は
環境ノイズや遮蔽や位置によって影響を受けやすい。
これモデルに入力する前に複数の前処理を挟むことで
複雑な屋内環境においてモデルの予測精度と信頼性を大幅に向上させことができる。
\subsubsection{キーポイントの抽出}
yolo-poseではキーポイントとして、鼻、目、耳、肩、肘、手首、股関節、膝、足首の
合計17種（鼻以外の部位は左右を持つ）が含まれる。
転倒と非転倒事象の判別において、特定の骨格関節は識別特性を持たないとみなされる。
処理の効率化と明瞭化のため、これらの関節は除去する。
また、このシステムの運用は介護施設や障害者施設を想定しており、
そのような場所では
マスクをつけている利用者が多い。
そのため、鼻のキーポイントの検出が難しい状況が想像できるので
鼻のキーポイントは除去し、代わりに左右の目の中央のキーポイントという新たな特徴量を作成する。
左右の目の中央のキーポイントはシンプルに左右の目の平均をとって算出した。
最終的に保持される訓練用関節セットには、目の中央、肩、肘、手首、股関節、膝、足首の合計
13種のキーポイントが含まれる。

\subsubsection{線形補完}
姿勢推定に基づく関節座標は，遮蔽や検出失敗により一部フレームで欠損が生じることがある。
本研究では，時系列特徴量の連続性を維持するため，学習時に検出に失敗した関節点に対して
線形補完を用いた欠損補完処理を行った。

まず，各動画から等間隔に抽出した30フレームについて関節座標を取得し，欠損キーポイントを探した。
欠損が存在する場合には，検出が成功した前後フレームの関節座標を用い，
各関節点の x および y 座標について時間軸方向の線形補間を行った。
フレーム時刻 t における関節 j の座標成分 $p_{j}^{(k)}(t)（k \in \{x, y\}）$が欠損している場合，
直前および直後の有効フレーム $t_1, t_2（t_1 < t < t_2）$を用いて，以下の線形補間式により推定した。

\begin{equation}
p_{j}^{(k)}(t)=p_{j}^{(k)}(t_1)+\frac{t - t_1}{t_2 - t_1}
\left(
p_{j}^{(k)}(t_2) - p_{j}^{(k)}(t_1)
\right)
  \label{eq:Linear-Interpolation}
\end{equation}

なお，動画の先頭および末尾において有効フレームが存在しない場合には，
最初または最後に検出された関節座標を用いた定数補完を行い，外挿による不安定な推定を防止した。

\subsubsection{相対位置正規化}
最小値-最大値正規化を適用してデータを区間[0, 1]にスケーリングし、
人物の位置による大小関係や人物の身長の差による精度の差を無くすことができる。

\begin{equation}
s = \frac{1}{N} \sum_{i=1}^{N} \left\| \mathbf{p}_i - \mathbf{p}_0 \right\|
\label{eq:human_scale}
\end{equation}

その後、すべての関節位置座標をスケールで割ってスケルトンのサイズを変更し、
正規化されたスケールを1にした．
正規化された関節位置$\hat{\mathbf{p}}_i$は次のように計算された．

「肩〜足首の距離」など特定の部位基準ではないので、学術的に厳密な「身長正規化」とは少し異なるが、
人物の大きさに依存しない形に正規化をした．

\begin{equation}
\hat{\mathbf{p}}_i = \frac{1}{s} (\mathbf{p}_i - \mathbf{p}_0)
\label{eq:joint_norm}
\end{equation}

また、各キーポイントを体の中心からの相対位置に変換することで、人物のフレームの中の位置によらす
正しく分類できるようにする。
体の中心を左右の股関節の中央の位置$(p_{HipCenter}^x,p_{HipCenter}^y)$とする。
すると、各座標の相対位置$(rp_{n}^x,rp_{n}^y)$は式\ref{eq:rx-n-ry-n}のように計算できる。

\begin{equation}
  \begin{aligned}
rp_{n}^x=(p_n^x-p_{HipCenter}^x)\\
rp_{n}^y=(p_n^y-p_{HipCenter}^y)
\label{eq:rx-n-ry-n}
  \end{aligned}
\end{equation}



\section{畳み込みニューラルネットワーク（CNN：Convolutional Neural Network）}
画像認識において頻繁に使われる畳み込みニューラルネットワーク（CNN：Convolutional Neural Network）は
畳み込み層やプーリング層、全結合層などから構成される．
そのため、まずニューロンモデルや畳み込み演算や特徴抽出などの用語や新たな概念、処理方法について説明する．

\subsubsection{ニューロンモデル}
ニューロンモデルとは脳の神経細胞（ニューロン：neuron）を模した数理モデルである．
これによって、データの予測や簡単な物体検出が可能になる．

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/neuron_model.png}
  \caption{ニューロンモデル}
  \label{fig:neuron_model}
\end{figure}

1つのニューロンに2つの入力$x_0$と$x_1$が入力信号として入ってくる（図 \ref{fig:neuron_model}）．
各入力と各入力に対する重み$w_0$と$w_1$を掛け算し、合計をとり、最後に定数$w_2$を加えた値を
入力総和$a$とする．
$x_{2}=1$とすると、入力総和$a$を式 \ref{eq:a}のように表せる．



\begin{equation}
  \begin{aligned}
a&=w_0x_0+w_1x_1+w_2x_2\\
&=\sum_{i=0}^2 w_{i}x_{i}
\label{eq:a}
\end{aligned}
\end{equation}

その後、この入力総和$a$を活性化関数（この例ではシグモイド関数）に通した式 \ref{eq:y}がニューロンの出力値yとなる．

\begin{equation}
  \begin{aligned}
y=\sigma(a)
=\frac{1}{1+exp(-a)}
\label{eq:y}
\end{aligned}
\end{equation}

このように，1つのニューロンは入力データ $\bm{x}$ に対して，
重み付き和と活性化関数による変換を行い，出力値 $y$ を生成する．
この出力値$y$は，入力データがあるクラスに属する確率や，
数値を予測した結果として解釈される．

\subsubsection{ニューロンモデルの学習方法}
本研究で扱うニューロンモデルの学習は，
入力データとそれに対応する正解ラベル（教師データ）が与えられる
教師あり学習に基づいて行われる．
学習時には、入力データ $\bm{X}$ をニューロンに入力し、ニューロンが出力したy（予測データ）と
教師データ$\bm{T}$の誤差が最小になるように
この重み$\bm{W}$（以下、パラメータという）を調整していく．

また、ニューロンが出力したy（予測データ）と元の教師データの誤差を
平均交差エントロピー誤差$E(\bm{w})$という（式 \ref{eq:average_cross_entropy_error}）．
この誤差関数は，予測結果 $y_n$ が教師データ $t_n$ に近づくほど小さな値をとる．

\begin{equation}
  \begin{aligned}
E(\bm{w})=-\frac{1}{N} \log P(\bm{T}|\bm{X})
=-\frac{1}{N} \sum_{n=0}^{N-1}{t_n log y_n + (1-t_n) log (1-y_n)}
\label{eq:average_cross_entropy_error}
\end{aligned}
\end{equation}


パラメータの偏微分（式 \ref{eq:derivative_of_the_mean_cross-entropy_error}）が
0になったときのパラメータが平均交差エントロピー誤差が最小になるパラメータだと考える．
\begin{equation}
\frac{\partial E}{\partial w_i}=\frac{1}{N} \sum_{n=0}^{N-1}(y_n - t_n) x_{ni}
\label{eq:derivative_of_the_mean_cross-entropy_error}
\end{equation}

パラメータの調整には勾配降下法を使って行う（式 \ref{eq:w}）．
誤差関数を各パラメータで偏微分することで，
誤差が増加する方向を求め，
その逆方向にパラメータを更新する方法である．
この操作をすべての学習データに対して繰り返すことで，
モデルは入力データと教師データの対応関係を学習する．
$\alpha$を学習率といい、一般的には0.01といった極めて小さい値となる．

\begin{equation}
w_i(\tau+1)=w_i(\tau) - \alpha \frac{\partial E}{\partial w_i}
\label{eq:w}
\end{equation}

\subsubsection{活性化関数}
活性化関数とはニューロンの入力総和から出力を決定するための関数である．
活性化関数は複数の種類があり、用途によって使い分ける．
ここでは活性化関数の中でも頻繁に使われるシグモイド関数とソフトマックス関数、ReUL関数を
説明したい．

\begin{itemize}
\item シグモイド関数 \mbox{}\\
負から生の実数をを0から1までの値に変換する関数である．
実数を確率に変換する際によく用いられる．
\begin{equation}
\sigma(x)=\frac{1}{1+exp(-x)}
\label{eq:sigmoid_func}
\end{equation}

\item ソフトマックス関数 \mbox{}\\
シグモイド関数の多次元に拡張した関数であるため、
多項分類器に使われることが多い．
\begin{equation}
h(x_k)=\frac{exp(x_l)}{\sum_{l=0}^{L}exp(x_l)}
\label{eq:softmax_func}
\end{equation}

\item ReUL関数 \mbox{}\\
入力が0以下なら0を、0より大きければその値をそのまま出力する関数である．
ニューラルネットワークの活性化関数で使われることが多い．
\begin{equation}
relu(x)=max(0,x)
\label{eq:reul_func}
\end{equation}

\end{itemize}

\subsubsection{ニューラルネットワーク}
単一のんツーロンモデルでは単純な分類しか行えないが、
複数のニューロンを層状に接続することで、より複雑な分類が可能になる．
このニューロンモデルの集合体をニューラルネットワークという．
ニューラルネットワークは主に入力層、中間層（隠れ層ともいう）、出力層に分かれる．
図 \ref{fig:neuarl_network}は2次元の入力（$x_2$はダミー入力のため数に含めない）を受け、
３つのカテゴリーに分ける多項分類モデルである．
入力層では観測データがそのまま入力され，
中間層では入力データの特徴を抽出する処理が行われる．
出力層では，中間層で得られた特徴量をもとに，
各クラスに属する確率が計算される．

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/neuron_network.png}
  \caption{ニューラルネットワーク}
  \label{fig:neuarl_network}
\end{figure}

入力層、中間層（隠れ層ともいう）、出力層の次元をそれぞれD、M、Kとすると、
ネットワークは次のように定義される．
\begin{align}
中間層の入力総和：b_j&=\sum_{i=0}^{D} w_{ji}x_i\\
中間層の出力:z_j&=h(b_j)\\
出力層の入力総和：a_k&=\sum_{j=0}^{M} v_{kj}z_j\\
最終的な出力：y_k&=\frac{exp(a_k)}{\sum_{k=0}^{K-1} exp(a_k)}
\label{eq:neural_network_eq}
\end{align}

最終的な出力（式 \ref{eq:neural_network_eq}）にはソフトマックス関数が
使われる．\\
CNNでは，中間層の一部を畳み込み層やプーリング層に置き換えることで，
画像の空間的特徴を効率的に抽出する．

\subsubsection{誤差逆伝搬法（バックプロパゲーション）}
ニューラルネットワークに学習を行わせる方法として、
主に誤差逆伝搬法（バックプロパゲーション）がある．
誤差逆伝搬法は、平均交差エントロピー誤差を使って、入力方向とは逆向きに
出力層の重みから入力層の重みへ順に更新していく方法である．
しかし、誤差逆伝搬法は新しい最適化手法ではなく，ニューロンモデルの学習方法である
勾配降下法をニューラルネットワークに
適用させた計算手順である．
実際に、勾配降下法と同じように平均交差エントロピー誤差関数を
パラメータ$w_{ji}$と$v_{ji}$で偏微分させた関数を使って
パラメータを更新する．

一つのデータnに対する交差エントロピー誤差$E_n$を式 
\ref{eq:cross_entropy_error_for_neural_network}のように定義すると、
平均交差エントロピー誤差は式\ref{eq:average_cross_entropy_error_for_neural_network}
のように簡潔に表せる．

\begin{align}
E_n(\bm{W},\bm{V})= -\sum_{k=0}^{K-1} t_{nk} log y_{nk}
\label{eq:cross_entropy_error_for_neural_network}
\end{align}
\begin{align}
E(\bm{W},\bm{V})
=-\frac{1}{N} \sum_{n=0}^{N-1}\sum_{K=0}^{K-1}{t_{nk} log y_{nk}}
= \frac{1}{N} \sum_{n=0}^{N-1} E_n(\bm{W},\bm{V})
\label{eq:average_cross_entropy_error_for_neural_network}
\end{align}

ここでは、式の見やすさのため$E_n$を$E$とし、nを省力する．
また、$E$を$v_{kj}$で偏微分した式が連鎖率より、
式 \ref{eq:derivative_of_average_cross_entropy_error_for_neural_network_v}
である．
$\delta_{k}$は、出力層の各ニューロンにおける出力$y_k$（予測データ）
とそれに対する教師データ$t_k$の
誤差の大きさを表す量であり，
誤差逆伝搬法における基本的な指標である．
この値を用いることで，
各重みが誤差に与える影響を定量的に評価できる．

\begin{equation}
  \begin{aligned}
\frac{\partial E}{\partial v_{kj}}
&=  \frac{\partial E}{\partial a_{k}}\frac{\partial a_k}{\partial v_{kj}}\\
&=(y_{k}-t_{k})z_j\\
&= \delta_{k}z_j
\label{eq:derivative_of_average_cross_entropy_error_for_neural_network_v}
\end{aligned}
\end{equation}

$v_kj$の更新規則はニューロンモデルと同じように
式 \ref{eq:updated_v_kj}になる．
\begin{equation}
  \begin{aligned}
v_{kj}(\tau +1)&=v_{kj}(\tau) - \alpha \frac{\partial E}{\partial v_{kj}}
&=v_{kj}(\tau)- \alpha \delta_{k} z_j
\label{eq:updated_v_kj}
\end{aligned}
\end{equation}
この意味は、$v_{kj}$の変化の大きさが最終的な誤差$\delta_{k}$と中間層の出力$z_j$の積で
決まるということである．
もし、はじめのニューロンの出力$y_n$がとそれに対する教師データ$t_n$が一致し、誤差$\delta_k$が0の場合は
$v_{kj}$は変化しない．
$z_{kj}$に対しては値が大きいほど、出力$y_k$への寄与が大きいとして、$v_{kj}$の変化量を
その分大きくするよう働く．

次に、Eの$w_{ji}$による偏微分を求める．
まず、連鎖率により分解することができる（式\ref{eq:w_ji_chain_rate}）．

\begin{equation}
\frac{\partial E}{\partial w_{ji}}=
\frac{\partial E}{\partial b_{j}} \frac{\partial b_j}{\partial w_{ji}}
\label{eq:w_ji_chain_rate}
\end{equation}

ここで、式 \ref{eq:updated_v_kj}の類似性からとりあえず、$\frac{\partial E}{\partial b_{j}}$を
式\ref{eq:delta_j}のように定義する．

\begin{equation}
\frac{\partial E}{\partial b_{j}}=\delta_{j}
\label{eq:delta_j}
\end{equation}

すると、連鎖率により、Eの$w_{ji}$による偏微分は
式 \ref{eq:derivative_of_average_cross_entropy_error_for_neural_network_w}である．

\begin{equation}
  \begin{aligned}
\frac{\partial E}{\partial w_{ji}}
=\frac{\partial E}{\partial b_{j}}\frac{\partial b_j}{\partial w_{ji}}\\
= \delta_j x_i
\label{eq:derivative_of_average_cross_entropy_error_for_neural_network_w}
\end{aligned}
\end{equation}


$\delta_j$について中間層の活性化関数を$h()$とすると、
具体的に式\ref{eq:delta_j_2}と表せる．
\begin{equation}
\delta_{j} =h'(b_j)\sum_{k=0}^{K-1}v_{kj} \delta_k
\label{eq:delta_j_2}
\end{equation}
$h'(b_j)$は活性化関数の微分を$\sum_{k=0}^{K-1}v_{kj} \delta_k$は
出力先の誤差を各重みで掛け合わせた総和を示している．
つまり、$\delta_{j}$は出力先での誤差を逆伝搬し、計算している（図 \ref{fig:back_propagation}）．
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/back_propagation.png}
  \caption{誤差の逆伝搬}
  \label{fig:back_propagation}
\end{figure}



$w_{ji}$は式 \ref{eq:updated_w_ji}に従って更新される．
この式から、$w_{ji}$も結合先の誤差$\delta_j$と結合元の出力$x_i$の積
によって変化していることがわかる．

\begin{equation}
  \begin{aligned}
w_{ji}(\tau +1)=w_{ji}(\tau) - \alpha \frac{\partial E}{\partial w_{ji}}
=w_{ji}(\tau)- \alpha \delta_{j} x_i
\label{eq:updated_w_ji}
\end{aligned}
\end{equation}

以上の計算をまとめると，
誤差逆伝搬法は以下の手順で学習を進める．
\begin{enumerate}
  \item ネットワークにxを入力し、出力層のニューロンごとにyを出力する．
\item 出力層のニューロンごとに出力yと教師データtの間の誤差を計算する．
\item 出力層の誤差から中間層の誤差を計算する．
\item 中間層の重みを結合元の重みや結合先の誤差を使って更新する．
\item 1〜4の作業を繰り消し、重みを改善していく．
\end{enumerate}



\subsection{カーネルと畳み込み演算}
CNNを説明するにあたって、二次元配列による画像の表現方法について、いくつか触れておく．
画像は画素（ピクセル）という小さい点によって格子状に構成されている．
そして、その各画素に割り当てられる色を表す値を画素値という．
画像の左上を原点$(0,0)$として右方向と下方向をそれぞれx軸とy軸の正方向とする．
また、各画素値に割り当てられる色を表す軸をチャネルと呼ぶ．
カラー画像は一般的に3チャネルの二次元配列でできており、3つの値はそれぞれ赤、緑、青の
色の強度を表している．
具体的にカラー画像の配列の形は$(512,512,3)$((画像の高さ,画像の幅,チャネル数))、
カラー画像の画素値を$(196,200,165)$((赤,緑,青))とされる．
前節までで説明した全結合型のニューラルネットワークは，
入力次元が大きくなるとパラメータ数が急増するという課題がある．
CNNではこの問題に対処するため，
局所的な領域に着目した平滑化フィルタや畳み込み演算を用いて
特徴抽出を行う．

平滑化フィルタとは画像の各画素値を周りの画素値を考慮した上で更新する方法である．
これを行うことで画像のノイズ除去が実現できる．
平滑化フィルタを行う際は、ある画素値に対してどのように周辺の画素値を
収集するか決める数学的な行列または配列であるカーネルを用いる．
また、カーネルには目的によって異なる行列や配列を用いる．
カーネルを用意したら、カーネルと画像との間で各画素値に対して、
図 \ref{fig:convolution}のような畳み込み演算を行う．
この処理を画像全体に施すことで、フィルタ処理が完了する．
\begin{figure}[tbp]
  \centering
  \includegraphics[scale=0.4]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/convolution.png}
  \caption{畳み込み演算}
  \label{fig:convolution}
\end{figure}
畳み込み演算は式 \ref{eq:convolution}のように表せる．
ここで、*は畳み込み演算の演算子を、$I_g(x,y)$と$I_o$、$K$はフィルタ適用後の画像と
元画像、カーネルを表し、WとHは画像の幅と高さを表している．

\begin{equation}
  I_g(x,y)=K * I_o=\sum_{u=-[W/2]}^{[W/2]} \sum_{v=-[H/2]}^{[H/2]} K(u+[W/2],v+[H/2])I_o(x+u,y+v)
\label{eq:convolution}
\end{equation}

\subsection{特徴抽出}
画像の畳み込み演算を使った特徴抽出について触れたい．
まずは微分を使ったエッジ特徴量の抽出から始める．
エッジ特徴量とは画素値の変化が大きい部分であり、これを行うことで物体の輪郭抽出が可能になる．
垂直方向のエッジ特徴量と水平方向のエッジ特徴量は式 \ref{eq:edge_feature_vertical_horizontal}で計算できる．
実際には、図 \ref{fig:kernel_for_feature_extraction}の（a）x軸方向の1次微分カーネルと
（b）y軸方向の1次微分カーネルを
使うことで計算できる．
\begin{gather}
  E_x(x,y)=\left| \frac{\partial}{\partial x}I(x,y) \right| \simeq \left| I(x+1,y)-I(x,y)\right| \\
  E_y(x,y)=\left| \frac{\partial}{\partial y}I(x,y) \right| \simeq \left| I(x,y+1)-I(x,y)\right|
\label{eq:edge_feature_vertical_horizontal}
\end{gather}

最終的に垂直方向と水平方向のエッジ特徴量を合わせて式 \ref{eq:edge_feature}の一次微分のエッジ特徴量を算出する．
\begin{equation}
  E(x,y)=\sqrt{E_x(x,y)^2+E_y(x,y)^2}	
\label{eq:edge_feature}
\end{equation}


\begin{figure}[h]
  \centering
  \includegraphics[scale=0.6]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/kernel_for_feature_extraction.png}
  \caption{特徴抽出用カーネル}
  \label{fig:kernel_for_feature_extraction}
\end{figure}

1次微分値をさらに微分した2次微分を計算することで、輝度の変化が大きい部分を抽出することが可能である．
座標$(x,y)$における2次微分を行ったエッジ特徴量$E_L(x,y)$は式\ref{eq:Second-order_differential_edge_features}になる．
２次微分の特徴抽出は図 \ref{fig:kernel_for_feature_extraction}の（c）ラプラシアンフィルタのカーネルを
使うことで計算できる．
\begin{equation}
  \begin{aligned}
  E_L(x,y)&=\delta I(x,y)  \\
  &=\frac{\partial ^2}{\partial x^2}I(x,y)+\frac{\partial ^2}{\partial y^2}I(x,y)  \\
  & \simeq \frac{\partial}{\partial x}{I(x+1,y)-I(x,y)}+\frac{\partial}{\partial y}{I(x,y+1)-I(x,y)}\\
  & \simeq {I(x+1,y)-I(x,y)}-{I(x,y)-I(x-1,y)}+{I(x,y+1)-I(x,y)}-{I(x,y)-I(x,y-1)}\\
  &=I(x+1,y)+I(x-1,y)+I(x,y+1)+I(x,y-1)-4I(x,y)
\label{eq:Second-order_differential_edge_features}
\end{aligned}
\end{equation}

このようにカーネルと畳み込み演算を使った処理によって画像から特徴量を抽出することができる．
この性質はCNNに利用されている．
CNNでは，このような特徴抽出を一つのカーネルだけで行うのではなく，
複数の異なるカーネルを同時に用いる．
各カーネルは異なるパターン（エッジ，角，テクスチャなど）に反応するため，
同一の入力画像に対して複数種類の特徴マップが生成される．
例えば，垂直方向のエッジを強調するカーネル，
水平方向のエッジを強調するカーネル，
斜め方向のエッジを検出するカーネルを同時に適用することで，
画像中の輪郭構造を捉えることが可能となる．

\subsection{畳み込み層}
畳み込み層では畳み込み演算を基本とする処理になる．
畳み込み層では出力チャネル数、カーネルの幅と高さ及びストライドはハイパーパラメータとなり、
カーネルはパラメータとなり、学習により決定される．
また、CNNではx軸とy軸に加えて、チャネル方向にも畳み込みを行う．
入力画像がRGBの3チャネルの場合、チャネルごとに
異なるフィルタを用意し、畳み込み演算した結果を全部加算し、
その結果を出力マップとする．
そのため、出力される特徴マップの数は入力のチャネル数に直接影響されない．
また、1つの特徴マップに複数のカーネルを用いることで，
出力される特徴マップのチャネル数は増加する．
このようにして，層を重ねるごとに
より多様で抽象的な特徴表現が獲得される．

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/two_convolution_layer.png}
  \caption{2層間の畳み込み演算}
  \label{fig:two_convolution_layer}
\end{figure}
図\ref{fig:two_convolution_layer}のように第l-1層が$C_{l-1}$チャネルで構成されている場合、
$C_{l-1}$層のカーネル高さ,カーネル幅,$C_{l-1}$チャネルの直方体に畳み込み演算を行い、第l層のある一点が計算される．\\

特徴マップとは、畳み込み演算により生成される[高さ,幅,チャネル数]の形の特徴量である．
ストライドとは畳み込み演算を適用する間隔である．\\
図 \ref{fig:stride}の通り、ストライドが２の場合は、
カーネルをx方向、またはy方向に2画素ずつずらして畳み込み演算を行い、結果の値を特徴マップに代入していく．
そのため、畳み込み演算適用前と畳み込み演算適用後の特徴マップは幅と高さが異なる．\\

畳み込み層では畳み込み演算のほかにパディングという処理も必要であることがある．
パディングとは特徴マップの端をある適当な値で埋める処理を指す．
特徴マップが削られることを防ぐことが目的である．また、パディングで埋める値は０であることが多い．


\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/stride.png}
  \caption{ストライド2の場合の畳み込み演算}
  \label{fig:stride}
\end{figure}

\subsection{プーリング層}
プーリング層では、カーネルの値をまとめる処理を行う．
プーリング処理には最大プーリングや平均プーリングなどが存在するが、
今回は最大プーリングのみ紹介する．
図 \ref{fig:max_pooling}はカーネルを[3,3]とし、ストライドを2とした場合の最大プーリングの処理の様子である．
カーネルと同じサイズの$C_{l-1}$層のブロックの中で最大の値を次の$C_l$層の画素の値とする．
プーリングはカーネルを移動させながら行うが、チャネルごとに独立して行われるなど、畳み込み層とは異なる部分もある．

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/max_pooling.png}
  \caption{最大プーリング}
  \label{fig:max_pooling}
\end{figure}
CNNでは、一般的に畳み込み層やプーリング層のストライドを用いて特徴マップを縮小しながら、特徴抽出を行う．
特徴マップを縮小させることで計算量を減らすことができる．

\subsection{全結合層}
全結合層は，畳み込み層やプーリング層によって抽出された局所的な特徴を
全体として統合し，最終的な判断を行う役割を担う．
畳み込み層が画像中の局所的なパターンを検出するのに対し，
全結合層はそれらの特徴の組み合わせから，
ニューラルネットワークとして学習された重みにより、
画像全体がどのクラスに属するかを判定する．
全結合層に入力するためには、畳み込み層やプーリング層の出力である
特徴マップを一次元に変換する必要があり、この処理をFlattenという．
この仕組みはYOLOにおいても同様であり、畳み込み層で抽出された
特徴量を全結合層を通じて人物の位置やクラス確率といった検出結果
へ変換するために用いられている．

\subsection{畳み込みニューラルネットワークの構成}
図 \ref{fig:cnn_composition}はCNNの基本的な構成の例である．
畳み込み層とプーリング層を相互に複数回適用しながらニューラルネットワークを構築し、
出力層の前に全結合層を用いた．
一回の畳み込み層ではなく、複数回の畳み込み層を適用することで、
単純な低次元特徴マップから、より複数の特徴量を含む高次元特徴マップの検出が
可能となる\cite{neural-network-introduction}．
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/cnn_composition.png}
  \caption{CNNの構成}
  \label{fig:cnn_composition}
\end{figure}


\section{YOLOによる物体検知}
本研究では物体検知手法として YOLO（You Only Look Once）を用いる。
YOLO は高速かつ高精度な物体検出手法であり，現在までに v11 まで発展している。
本節では，YOLO の基本的な原理を理解するため，
最初に提案された YOLO v1 の検出アルゴリズムを中心に説明する。
従来の物体検出では、分類器を利用して行ってが、YOLOは
物体検出をバウンディングボックスとそれに関連するクラス確率への回帰問題として扱う。
単一のニューラルネットワークが、1回の評価で画像全体からバウンディングボックスとクラス確率を直接予測できる。
各バウンディングボックスは、x、y、w、h、および信頼度の5つの予測値で構成される。
物体を検知する原理としては,まず入力された画像をS×S個の正方形（グリッドセル）に分割する．
物体の中心がグリッドセルに収まる場合、そのグリッドセルがその物体の検出を担当する。
画像分類における予測確率を信頼度として、モデルがそのバウンディングボックスに物体が含まれていると予想した確率
$Pr(Object)$とボックスの位置の正確さを表した$IOU_{\frac{\text{truth}}{\text{pred}}}$を掛け合わせた
$Pr(Object) * IOU_{\frac{\text{truth}}{\text{pred}}} $を定義する。
ここで定義される信頼度は学習時の目標値であり，
推論時にはネットワークが IOU を直接計算するのではなく，
IOU を近似する値として信頼度を予測する。

各グリッドセルはB個の、物体を囲んだ部分領域であるバウンディングボックスと
各信頼度スコアを予測する．
信頼度スコアは0に近いほど,背景であることを示し,1に近いほど,物体であることを示す．
各グリッドセルは、C個の条件付きクラス確率Pr(Classi|Object)も予測する。
式 \ref{eq:confidence_score}のように各バウンディングボックスは、
「中心座標を含むセルに物体がある」という条件付きのクラス確率
$Pr(Class_i | Object) $とボックスの正確さを示す信頼度
$Pr(Object) * IOU_{\frac{\text{truth}}{\text{pred}}}$を
掛け合わせて,クラス固有の信頼度スコアが得られる．



\begin{equation}
  \begin{aligned}
    Pr(Class_i | Object) * Pr(Object) *  \\
    IOU_{\frac{\text{truth}}{\text{pred}}} 
    &= Pr(Class_i) * IOU_{\frac{\text{truth}}{\text{pred}}}
    \label{eq:confidence_score}
  \end{aligned}
\end{equation}

このスコアは,そのクラスがボックスに出現する確率と,
予測されたボックスがオブジェクトにどれだけ適合するかの両方を表す．

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.15]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/yolo_model.png}
  \caption{モデルの検出フロー}
  \label{fig:yolo_model}
\end{figure}
最後にNon-Maximum Suppression（NMS） を用いて,重複するバウンディングボックスの中から,
最終的にクラス別に最も正確なバウンディングボックスを選び抜く\cite{openflow-acm}。


\subsection{IoU（intersection over Unio）}
物体検出における物体の位置は、物体の上端と下端に水平の線を、右端と左端に垂直な線を引いて得られる長方形である
外接矩形（bounding box）で表される．
矩形は矩形の左上端点を原点として右方向をx軸の正方向、下方向をy軸の正方向とする．
矩形の大きさは幅をw、高さをhと表す．
外接矩形の表現方法として2通りある．一つ目は中心座標$(x,y)$と大きさ$(w,h)$で表す方法である．
二つ目は矩形の左上端点の座標$(x_{min},y_{min})$と右下端点の座標$(x_{max},y_{max})$で表す方法である．

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/bounding_box.jpg}
  \caption{物体検出で使われる外接矩形}
  \label{fig:bounding_box}
\end{figure}

物体検出器が予測する外接矩形は位置に誤差が生じるため、必ずしも実際の物体の位置と重なるとは限らない．
より物体の位置を正確にするため、実際の物体の位置と予測される外接矩形の当てはまり度合いに
IoU（intersection over Union）という指標を使います．
具体的には学習時に損失関数を計算したり、評価時に検出矩形が正しいか判定する用途で使われる．
IoUの計算方法は式 \ref{eq:IoU}の通りです．

\begin{equation}
IoU=\frac{積集合の面積}{和集合の面積} 
\label{eq:IoU}
\end{equation}
IoUは二つの矩形の和集合の面積に対しての積集合の面積の割合さと言える．
IoUは2つの矩形の中心位置が近く、かつ大きさが等しいほど、大きくなる．

\subsection{Non-Maximum Suppression（NMS）}
Non-Maximum Suppression（NMS）とは１つの物体に対して
複数のバウンディングボックスが存在するとき、最も正確なボックスを
選び、その他の余計なボックスを取り除くためのアルゴリズムである。
まず、すべてのボックスをスコアに基づいてソートする。
最大スコアを持つボックスMが選択され、その他のボックスとのIOUが計算される。
IOUが事前定義された閾値を超えた場合に、その二つのボックスは同じものを検知しているとして、
重複しているボックスは削除される\cite{NMS}。
この作業をくり返していくことで、余計なボックスを取り除いていく。

\subsection{ネットワーク設計}
本研究では YOLO v1 のネットワーク構成を示す（図 \ref{fig:network_design}）。
ネットワークは24層の畳み込み層と、それに続く2層の全結合層で構成されており、
$-s-$はストライド間隔を示している。
ネットワークの最初の畳み込み層は画像から特徴を抽出し、
全結合層は出力確率と座標を予測する。
最終的な出力は、7×7×30の多次元配列です。
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.4]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/network_design.png}
  \caption{ネットワーク設計}
  \label{fig:network_design}
\end{figure}

\section{YOLO-Poseによる姿勢推定}
YOLO-Poseは物体検出モデルであるYOLO v5を基盤とし,画像における共同検出と2D複数人物ポーズ推定を行う
手法である。
具体的には、検出された人物ごとに複数のキーポイント（関節点）の位置と信頼度を回帰的に推定する．
そのため、YOLO-Poseは2つの異なる出力を持っており、
一つはバウンディングボックスを予測し,もう一つは人の骨格を構成するキーポイントを推定する。
人物のすべてのキーポイントをアンカーに関連付けられる。\\
一般的に既存の姿勢推定には、トップダウン方式とボトムアップ方式の2つに分類される。
yolo-poseではトップダウン方式とボトムアップ方式の両方の長所を取り入れている。

学習時には、正解となるバウンディングボックスに一致するアンカーボックス
またはアンカーポイントは、バウンディングボックスの位置とともに、
2次元姿勢を保存する。

異なる人物の2つの類似した関節は、空間的に互いに近接している可能性がある。
従来のボトムアップ方式で用いられるヒートマップを使用すると、
異なる人物の空間的に近い2つの類似した関節を区別することは困難である。

しかし、これら2人の人物を異なるアンカーと照合すれば、
空間的に近接した類似のキーポイントを区別するのは容易になる。
アンカーに関連付けられたキーポイントは既にグループ化されているため、
さらにグループ化する必要はない。
このように、トップダウン方式の人物単位の識別能力とボトムアップ方式の一定時間での処理という
両方の利点を導入することで、
一定の実行時間とシンプルな後処理を両立できる。\\

各人物には17個のキーポイントがあり、各キーポイントは座標と信頼度で識別される:$\{x,y,conf\}$。
したがって、各アンカーについて、キーポイントヘッドは51（$17 \times 3$）個の要素を予測し、
ボックスヘッドは6個の要素（x座標、y座標、幅、高さ、物体の信頼度、人クラスの信頼度）を予測する。
n個のキーポイントを持つアンカーの場合、全体的な予測ベクトルは次のように定義される。

\begin{equation}
  \begin{aligned}
  C_x,C_y&:中心座標 \quad W, H:幅・高さ\\
  box_{conf}&:物体の信頼度 \quad class_{conf}:人クラスの信頼度\\
  K_x^n,K_y^n&:各キーポイントの座標 \quad K_{conf}^n:各キーポイントの信頼度\\
P_v&=\{C_x,C_y,W,H,box_{conf},class_{conf},K_x^1,K_y^1,K_{conf}^1,...,K_x^n,K_y^n,K_{conf}^n\}
\label{eq:P_v}
\end{aligned}
\end{equation}

キーポイントの信頼度は、そのキーポイントの可視性フラグ$\delta(v_n)$に基づいて学習される。
キーポイントが可視または遮蔽されている場合、信頼度は1に設定され、
それ以外の場合、視野外にある場合は信頼度は0に設定されます。
推論中は、信頼度が0.5を超えるキーポイントを保持され、それ以外の予測キーポイントはすべて除外される。\\

また,YOLO-Pose では損失関数として,バウンディングボックスに対しては IOU の拡張である CIOU 損失を用い,
キーポイントに対しては Object Keypoint Similarity (OKS) に基づく損失を導入している．

ネットワーク構成としては，図 \ref{fig:yolo-pose-netwprk}の通りになる。\\
CSP-Darknet53をバックボーンとし，画像の特徴抽出を行い、
様々なスケール{P3、P4、P5、P6}で特徴マップを出力する。
次のPANetはこれらの複数スケールの特徴マップから人物検出及び姿勢推定に適した特徴表現を生成する。
特に，キーポイント推定では局所的な位置精度が重要であるため，
PANetによるマルチスケール特徴融合は重要な役割を果たす。
PANetの出力は検出ヘッドに送られます。
最終的に、異なるスケールに対応した4つの検出ヘッドはバウンディングボックスと
キーポイントを予測する。
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.3]{/Users/horikawafuka2/Documents/class_2025/sotuken/report/images/yolo-pose.png}
  \caption{yolo-poseアーキテクチャ}
  \label{fig:yolo-pose-netwprk}
\end{figure}

\subsection{トップダウン方式}
トップダウン方式[8], [12], [13], [19], [20], [21]、
または2段階アプローチでは、高機能な人物検出器を用いて人物検出を行い、
次に検出された人物ごとに2次元姿勢を推定する。
この方式では，各人物が独立に処理されるため，
人物同士が近接している場合や重なりが生じている場合でも，
比較的高精度な姿勢推定が可能である。\\
一方で，画像内の人物数が増加すると，
人物検出および姿勢推定の回数が増えるため，
計算量が人物数に比例して増大するという欠点を持つ。
そのため、トップダウン方式は複雑なタスクや高い精度が必要な場合に有効であり、
画像内に存在する物体の位置や形状を正確に検出できるという利点がある。

\subsection{ボトムアップ方式}
ボトムアップ方式[4], [5], [14], [23], [25]は、
画像内の人物全員の識別情報のないキーポイントを1回の撮影で見つけ出し、
それらを個々の人物インスタンスにグループ化する。
ボトムアップ方式は、ヒートマップと呼ばれる
確率マップに基づいて動作し、
各ピクセルが特定のキーポイントを含む確率を推定する。
ボトムアップ方式は、一般的に複雑さが少なく、
実行時間が一定であるという利点がある。\\
しかし，異なる人物の同種のキーポイントが
空間的に近接している場合，
ヒートマップに基づく識別が困難となり，
誤ったグループ化が生じやすいという課題がある。
また、トップダウン方式と比較すると、精度が大幅に低下する。

\subsection{IOUベースのバウンディングボックス損失関数}
現代の物体検出器のほとんどは、ボックス検出において、距離ベースの損失ではなく、
GIoU [36]、DIoU、CIoU [37] といったIoU損失の高度な変種を採用している。
バウンディングボックスの検出にはCIOU損失を採用しており、次のように定義される
（式 \ref{eq:bounding_box_func}）。
$Box_{pred}^{ijk}$は、位置(i ,j)とスケールsにおけるアンカーの予測ボックスである。
\begin{equation}
  \begin{aligned}
\mathcal{L}_{box}(s,i,j,k)=(1-CIOU(Box_{gt}^{s,i,j,k},Box_{pred}^{s,i,j,k}))
\label{eq:bounding_box_func}
\end{aligned}
\end{equation}

\subsection{Object Keypoint Similarity(OKS)}
yolo-poseではキーポイントを評価するための最も一般的な指標としてOKSが用いられる。
OKSは、物体検出におけるIOU（Intersection over Union）に相当する指標として機能し、
推定された姿勢が正解の姿勢とどれほど似ているかを0から1の値で表す。
OKSは各キーポイントごとに個別に計算され、その後合計されて最終的なOKS損失
またはキーポイントIOU損失が得られる（式 \ref{eq:OKS}）。
\begin{equation}
  \begin{aligned}
  d_n&=n番目のキーポイントの予測座標と正解座標とのユークリッド距離\\
  k_n&=キーポイントごとに設定される定数（推定が難しいものほど大きい値になる）\\
  s&=人の大きさ\\
  \delta(v_n)&=各キーポイントが画像に写っているか示すフラグ\\
\mathcal{L}_{kpts}(s,i,j,k) &= 1 - \sum_{n=1}^{N_{kpts}} OKS\\
&=\sum_{n=1}^{N_{kpts}} \frac{exp(\frac{d_n^2}{2s^2 k_n^2}) \delta(v_n > 0)}{\sum_{n=1}^{N_{kpts}} \delta (v_n > 0)} 
\label{eq:OKS}
\end{aligned}
\end{equation}

\section{YOLO-Trackingによる物体追跡}
本研究では1人ではなく、複数人を対象に転倒を検知するシステムのため、
物体追跡技術であるYOLO-TrackingのBoT-SORTを用いる。
YOLO-TrackingはYOLOによって各フレームの人物を抽出し、その検出結果をもとに
時系列方向で人物IDを対応付けることで
マルチオブジェクトトラッキング（Multi-Object Tracking, MOT）
を実現する手法である。
BoT-SORT\cite{BoT-SORT}には2022年6月に発表された物体追跡の手法であり、
既存の手法、ByteTrack\cite{ByteTrack}に以下の3つの変更を加えたものである。

\begin{enumerate}
\item カルマンフィルタ
\item カメラモーション補償（CMC）
\item IoU ReID フュージョン
\end{enumerate}

\subsection{マルチオブジェクトトラッキング（Multi-Object Tracking, MOT）}
一般的にMOTでは2段階に分けて処理を行う。
 MOT
1段階目は物体検出を行い、2段階目でデータの関連付けを行う。
第1段階では物体検出を行い，各フレームにおいて
人物などの対象物体のバウンディングボックスを取得する。
第2段階ではデータアソシエーションを行い，
異なるフレーム間の検出結果を対応付けることで，
時系列に沿った軌跡（トラック）を生成する。
データアソシエーションでは、まずトラックレット（動画内の移動物体が一時的に追跡された短い軌跡）と
検出ボックス間の類似度を計算し、
類似度に応じてオブジェクトに識別子を割り当てる。
この類似度には位置情報、動き情報、外観情報が含まれている。
特に物体検知による追跡に基づくMOTでは，
物体検出の精度が追跡性能の上限を大きく左右する。
遮蔽やモーションブラーが発生すると検出スコアが低下し，
その結果，追跡が途切れる問題が生じやすい。
この問題に対処するため，
近年のMOT手法では，
低信頼度の検出結果も含めて活用し，
より安定したデータアソシエーションを実現する試みが行われている \cite{ByteTrack}。

\subsection{Bytetrack}
ByteTrack では，BYTE（Binary Association of Every detection）と呼ばれる
シンプルかつ汎用的なデータ関連付け手法を提案している。
BYTEは高スコア検出ボックスのみを保持する従来の手法[33, 47, 69, 85]とは異なり、
ほぼすべての検出ボックスを保持し、それらを高スコアと低スコアに分離する。
まず、高スコア検出ボックスをトラックレットに関連付ける。
次に、低スコア検出ボックスと一致しないトラックレットを関連付けることで、
低スコア検出ボックス内のオブジェクトを復元し、同時に背景を除去する。
% 具体的に処理の流れを説明する。\\
% BYTEの入力は、ビデオシーケンス$\texttt{V}$と、物体検出器$\texttt{Det}$である。
% BYTEの出力はビデオのトラック$\mathcal{T}$で、各トラックには、
% 各フレームの物体の境界ボックスとIDが含まれる。
% ビデオの各フレームについて、検出器$\texttt{Det}$を使用して検出ボックスとスコアを予測する。
% すべての検出ボックスは、スコアが検出スコア閾値$\tau$より高いボックスは$\mathcal{D}{high}$に、
% 低い検出ボックスは$\mathcal{D}{low}$にに分類する。\\
% その後、カルマンフィルタを用いて、$\mathcal{T}$内の各トラックの現在のフレームにおける新しい位置を予測する。
% 最初の関連付けは、高スコア検出ボックス$\mathcal{D}{high}$と
% すべてのトラック$\mathcal{T}$との間で行われる。
% 最初の関連付けに使われる類似度は、検出ボックス$\mathcal{D}{high}$と
% 予測トラックボックス$\mathcal{T}$間のIoUまたはRe-ID特徴距離によって計算される。
% 次に、ハンガリーアルゴリズム[31]を用いて、類似度に基づいて高スコア検出ボックス$\mathcal{D}{high}$と
% すべてのトラック$\mathcal{T}$との
% マッチングを行う。
% 一致しない検出は$\mathcal{D}{remain}$に保持し、一致しないトラックは$\mathcal{T}_{remain}$に
% 保持する。
% この段階でマッチしなかったトラックは，遮蔽や動く被写体やカメラの動きによって生じる「ブレ」
% などの影響を受けている可能性が高い。\\
% 2回目の関連付けは、低スコア検出ボックス$\mathcal{D}{low}$と最初の関連付け後に残った
% トラック$\mathcal{T}_{remain}$との間で行われます。
% 低スコア検出ボックスは外観情報が不安定なので
% 2回目の関連付けでは類似度として外観の特徴は用いずにIoUのみを使用する。

% 一致しないトラックは$\mathcal{T}_{re-remain}$に保持し、
% 一致しない低スコア検出ボックスは背景とみなし、すべて削除する。
% 第二段階で一致しないトラック$\mathcal{T}_{re-remain}$はすぐに削除されるのではなく、
% 各トラックが特定のフレーム数を
% 超えて一致しない場合にのみ、トラック$\mathcal{T}$から削除する。
% これにより、短時間の遮蔽からの復活やIDの抑制が可能となる。
% 一方，第1段階で未対応となった高信頼度検出ボックス $\mathcal{D}_{remain}$ からは，新しいトラックが初期化される。

\subsection{カルマンフィルタ}
SORT [3] では、状態ベクトルは 7 つの要素、$x=[x_c,y_c,a,s, \dot x_c,\dot y_c,\dot s]^T$ と
されていました。
ここで、$(xc, yc)$は画像平面における物体中心の 2D 座標であり、
s はバウンディングボックスのスケール（面積）、
a はバウンディングボックスのアスペクト比である。
最近のトラッカーでは、状態ベクトルは 8 つの要素、
$x=[x_c,y_c,a,h, \dot x_c,\dot y_c,\dot a ,\dot h]^T$に
変更されている。
しかし、実験により、バウンディングボックスの幅と高さを直接推定する方が性能が向上することがわかった。
したがって、KFの状態ベクトルを式\ref{eq:state_vector}のように定義し、
測定ベクトルを式\ref{eq:measurement_vector}のように定義する。


\begin{equation}
  x_k=[x_c(k),y_c(k),w(k),h(k),\dot x_c(k),\dot y_c(k),\dot w(k),\dot h(k)]^\mathrm{T}
\label{eq:state_vector}
\end{equation}

\begin{equation}
z_k=[z_{x_c}(k),z_{y_c}(k),z_w(k),z_h(k)]^\mathrm{T}
\label{eq:measurement_vector}
\end{equation}
KFの修正は、バウンディングボックスの幅と物体の適合性の向上に寄与している。




\subsection{カメラモーション補償（CMC）}
カメラモーション補償（CMC）は不要なカメラの動き（揺れ、パン、傾き）を修正したり、
ぼやけや揺れを軽減する一連の技術である。
検出による追跡は、予測されたトラックレットの境界ボックスと検出された
境界ボックスの重なりに大きく依存し、データの関連付けを行う。
カメラが動いている場合、画像平面上の境界ボックスの位置は
劇的に変化する可能性があり、IDスイッチ（フレーム内で
追跡している物体の識別IDが、本来とは異なる別の物体に誤って割り当てられてしまう現象）や
などにつながる可能性がある。
これらを防ぐためにCMC機能を追加する。
追加のセンサ情報を用いず，隣接フレーム間の画像レジストレーションによって
カメラの剛体運動を推定する。
具体的には，OpenCV の Video Stabilization モジュールに基づき，
Shi–Tomasi 法による特徴点抽出と疎なオプティカルフロー追跡を行い，
RANSAC を用いてフレーム k-1 から k へのアフィン変換行列 
$\vb{A}_{k-1}^k \in \mathbb{R}^{2\times3}$を推定する。
推定されたアフィン変換は，Kalman Filter の状態空間に拡張して適用される。
回転・スケール成分を含む行列 $\(\tilde{\vb{M}}_{k-1}^k\)$と並進成分 
$\(\tilde{\vb{T}}_{k-1}^k\) $を定義し，予測状態および共分散行列を
次式により補正する。

\begin{equation}
\[
\hat{\vb*{x}}^\prime_{k|k-1}
=
\tilde{\vb*{M}}_{k-1}^k \hat{\vb*{x}}_{k|k-1}
+
\tilde{\vb*{T}}_{k-1}^k
\]

\[
{{\vb*{P}}}^{\prime}_{k|k-1}
=
\tilde{\vb*{M}}_{k-1}^k
{\vb*{P}}_{k|k-1}
{\tilde{\vb*{M}}_{k-1}^{k}}^\top
\]
\label{eq:kf}
\end{equation}


\subsection{IoU ReID フュージョン}
BoT-SORTでは、動き情報と外観情報を統合してデータの関連付けを向上させている。
動き情報$IoU$と外観情報$Re-ID$を統合する新しい手法として、
IoU 距離行列とコサイン距離行列を開発しました。
まず、IoU スコアでコサイン類似度が低い候補や距離が遠い候補を除外します。
次に、

\begin{equation}
    \begin{aligned}
    \hat{d}_{i, j}^{cos} = 
        \begin{cases}
                0.5 \cdot d_{i, j}^{cos}, \text{($d_{i, j}^{cos} < \theta_{emb})$ $\land$ $(d_{i, j}^{iou} < \theta_{iou})$}\\
            1, \text{otherwise}
        \end{cases}
    \end{aligned}
    \label{eq:masked_dist}        
\end{equation}
\begin{equation}
    C_{i, j} = min\{d_{i, j}^{iou}, \hat{d}_{i, j}^{cos}\}
    \label{eq:min_dist}        
\end{equation}
$d_{i, j}^{iou}$ はトラックレット $i$ 番目の予測境界ボックスと
 $j$ 番目の検出境界ボックス間の IoU 距離であり、モーション コストを表す。
 $d_{i, j}^{cos}$ は平均トラックレット外観記述子 $i$ と
 新しい検出記述子 $j$ 間のコサイン距離、つまり新しい外見コストである。
 $\theta{iou}$ はモーションコストの閾値で、0.5 に、$\theta_{emb}$は外見コストの
 閾値で0.25に設定されている。
 両方が閾値を超えない場合のみ、外観コストを用いて、
 それ以外は不一致として扱う。
 トラックレットと検出のありそうもないペアを拒否するために使用されます。
そして最終的なコスト行列の各要素は，IoU 距離と修正後の外観距離の最小値として定義される（\ref{eq:min_dist}）。

% --------------------------------------------------
% 第4章 ○○○○
% --------------------------------------------------
\chapter{実験方法}

\section{データセット}

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

表\ref{tb:env} に実験環境を示す．

\begin{table}[tbp]
  \centering
  \caption{実験環境}
  \label{tb:env}
  \begin{tabular}{ll}
    \hline
    項目 & 内容 \\
    \hline
    コントローラ & Ryu Controller \\
    スイッチ     & Open vSwitch \\
    トポロジ     & 3リンク構成 \\
    トラフィック生成 & iperf3 \\
    \hline
  \end{tabular}
\end{table}


○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

\section{実験条件}

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

図\ref{fig:delay} に遅延比較結果を示す．

\begin{figure}[tbp]
  \centering
  \fbox{\rule{0pt}{40mm}\rule{0.7\linewidth}{0pt}}
  \caption{遅延の比較}
  \label{fig:delay}
\end{figure}

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

\section{評価指標}

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
% --------------------------------------------------
% 第5章 ○○○○
% --------------------------------------------------
\chapter{実験結果}
\section{サンプル数と精度の関係}
\section{クラス別性性能評価}
\section{線形補完の効果比較}
\section{判定に有効な距離や人数によるタイムラグ}
\section{転倒検知精度の考察}
% --------------------------------------------------
% 第6章 考察
% --------------------------------------------------
\chapter{考察}
\section{スケール正規化の有効性}
\section{誤検知・未検知の分析}
\section{実運用時の課題}
% --------------------------------------------------
% 第7章 結論
% --------------------------------------------------
\chapter{結論}

本研究では SDN を用いた動的ルーティングにより，
ネットワーク混雑時の遅延改善が期待できることを示した．
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
今後は大規模ネットワークや異なるトラフィックパターンへの拡張を検討する．

% --------------------------------------------------
% 参考文献（指定形式）
% --------------------------------------------------
\renewcommand{\bibname}{参考文献}
\addcontentsline{toc}{chapter}{参考文献}

\begin{thebibliography}{99}
\bibitem{neural-network-introduction}
李銀星.山田和範.探検データサイエンス　ニューラルネットワーク入門.
共立出版株式会社,2024,226,探検データサイエンス
\bibitem{NMS}
Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi.
Soft-NMS -- Improving Object Detection With One Line of Code.
Computer Vision and Pattern Recognition (cs.CV),2017.4

\bibitem{yolo}
Navaneeth Bodla. Bharat Singh, Rama Chellappa, Larry S. Davis.
You Only Look Once:Unified, Real-Time Object Detection.
Computer Vision and Pattern Recognition (cs.CV),2015.6

\bibitem{yolo-pose}
Debapriya Maji, Soyeb Nagori, Manu Mathew, Deepak Poddar.
YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss.
Computer Vision and Pattern Recognition (cs.CV),2022.4

\bibitem{ByteTrack}
Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, Xinggang Wang.
ByteTrack: Multi-Object Tracking by Associating Every Detection Box.
Computer Vision and Pattern Recognition (cs.CV),2021.10

\bibitem{BoT-SORT}
BoT-SORT: Robust Associations Multi-Pedestrian Tracking.
Nir Aharon, Roy Orfaig, Ben-Zion Bobrovsky.
Computer Vision and Pattern Recognition (cs.CV),2022.6

\bibitem{MOT}
Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, Tae-Kyun Kim.
Multiple Object Tracking: A Literature Review.

\bibitem{racking}
Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, Ben Upcroft.
SIMPLE ONLINE AND REALTIME TRACKING.
Computer Vision and Pattern Recognition (cs.CV),2016.2

\bibitem{sdn-survey}
B.~A.~A.~Nunes et al.,
``A Survey of Software-Defined Networking,''
IEEE Communications Surveys and Tutorials, Vol. 16, No. 3, pp. 1617-1634, 2014.

\bibitem{openflow-acm}
N.~McKeown, T.~Anderson, H.~Balakrishnan, G.~Parulkar, L.~Peterson, J.~Rexford, S.~Shenker, and J.~Turner,
``OpenFlow: Enabling Innovation in Campus Networks,''
ACM SIGCOMM Computer Communication Review,  
Vol. 38, No. 2, pp. 69-74, 2008. 

\end{thebibliography}

% --------------------------------------------------
% 謝辞
% --------------------------------------------------
\chapter*{謝辞}

○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○
○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○○

% --------------------------------------------------
% 付録
% --------------------------------------------------
\appendix

\chapter{SDNコントローラ設定例}

\begin{verbatim}
# Ryu Controller sample
class SimpleSwitch(app_manager.RyuApp):
    def _packet_in_handler(...):
        # install flow dynamically
\end{verbatim}

\chapter{追加評価}

\begin{figure}[tbp]
  \centering
  \fbox{\rule{0pt}{40mm}\rule{0.7\linewidth}{0pt}}
  \caption{追加評価の図}
  \label{fig:extra}
\end{figure}

\end{document}
